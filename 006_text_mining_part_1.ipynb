{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "006_text_mining_part_1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrybrew/socialmediaanalytic/blob/master/006_text_mining_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITA7F96h3Jet",
        "colab_type": "text"
      },
      "source": [
        "# Text Mining Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Weq4-cVk0i",
        "colab_type": "text"
      },
      "source": [
        "Text mining is a process of exploring sizeable textual data and find patterns. Text Mining process the text itself. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining. \n",
        "\n",
        "Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithm used to classify information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Who5fcwfvaIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Library\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XTF7_OkUyRH",
        "colab_type": "text"
      },
      "source": [
        "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, opensource, easy to use, large community, and well documented. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analysis, preprocess, and understand the written text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25jtxy_RUj7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import nltk\n",
        "nltk.download('punkt') # Sentence Tokenizer\n",
        "nltk.download('stopwords') # Stopword\n",
        "nltk.download('wordnet') # Wordnet Dictionary for Lemmatization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAgHK0Mnf1wv",
        "colab_type": "text"
      },
      "source": [
        "## Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfIO62ScWbXt",
        "colab_type": "text"
      },
      "source": [
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UpC2JolU_jY",
        "colab_type": "text"
      },
      "source": [
        "###**1. English Text Pre-Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhCoryR1V1i6",
        "colab_type": "text"
      },
      "source": [
        "Here we preprocess the simple word below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGSkSUS1Vzk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input English Text\n",
        "text_en = 'The death toll from the coronavirus has reached 28 in South Korea with 600 newly confirmed cases, raising the national Itally to 4,812 cases, the South Korean Centers for Disease Control and Prevention (KCDC) said in a news release Tuesday.'\n",
        "text_en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edMbDC4odWqs",
        "colab_type": "text"
      },
      "source": [
        "##### **a. Remove Symbol and Character**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHpObEQaZH5B",
        "colab_type": "text"
      },
      "source": [
        "Remove symbol and character from the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtSNnHuAdb3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import string \n",
        "\n",
        "# Remove Symbol and Character\n",
        "text_en_nosymbol = text_en.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "text_en_nosymbol"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_uwT7EVk8S",
        "colab_type": "text"
      },
      "source": [
        "##### **b. Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRE7Yd8rWqSc",
        "colab_type": "text"
      },
      "source": [
        "Tokenization is the process of breaking a document down into words, punctuation marks, numeric digits, etc. We can do sentence tokenization and word tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHnq0MPJZbbZ",
        "colab_type": "text"
      },
      "source": [
        "Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI9U7NG9U1TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module \n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenize Sentence\n",
        "text_en_tokenizeds =sent_tokenize(text_en_nosymbol)\n",
        "\n",
        "# Show Tokenized Sentence\n",
        "text_en_tokenizeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp7cNx3CZe6f",
        "colab_type": "text"
      },
      "source": [
        "Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX-ym7GSVkjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word Tokenization\n",
        "\n",
        "# Import Module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize Word\n",
        "text_en_tokenizedw = word_tokenize(text_en_nosymbol)\n",
        "\n",
        "# Show Tokenized Sentence\n",
        "text_en_tokenizedw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppeneYMaWb4P",
        "colab_type": "text"
      },
      "source": [
        "##### **c. Remove Stopword**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg-j7uhaaPsW",
        "colab_type": "text"
      },
      "source": [
        "Though \"stop words\" usually refers to the most common words in a language. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as \"The Who\", \"The The\", or \"Take That\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6pW7G7wXEle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download English Stopwords from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Show Stopwords\n",
        "stopwords_en = nltk.corpus.stopwords.words('english')\n",
        "stopwords_en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOto8ctW3-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing Stopwords\n",
        "text_en_filtered =[]\n",
        "for w in text_en_tokenizedw:\n",
        "    if w not in stopwords_en:\n",
        "        text_en_filtered.append(w)\n",
        "\n",
        "# Show Tokenized vs Filtered\n",
        "print('Tokenized:',text_en_tokenizedw)\n",
        "print('Filtered:',text_en_filtered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3rd6uyoW_xP",
        "colab_type": "text"
      },
      "source": [
        "##### **d. Text Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Lqx79cWwAF",
        "colab_type": "text"
      },
      "source": [
        "Text normalization considers another type of noise in the text. For example connection, connected, connecting word reduce to a common word \"connect\". It reduces derivationally related forms of a word to a common root word.\n",
        "\n",
        "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjzKWT7bf9S",
        "colab_type": "text"
      },
      "source": [
        "***Stemming***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecmB7mfnbhzW",
        "colab_type": "text"
      },
      "source": [
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. Example : consulting -> consult, parties -> parti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVajLjzXNlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Set Stemming Function\n",
        "stemmer = PorterStemmer()\n",
        "text_en_stemmed =[]\n",
        "for i in text_en_filtered:\n",
        "    text_en_stemmed.append(stemmer.stem(i))\n",
        "\n",
        "# Show\n",
        "print('Filtered:',text_en_filtered)\n",
        "print('Stemmed:',text_en_stemmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqS0q87eN2W",
        "colab_type": "text"
      },
      "source": [
        "***Lemmatization***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTTpa9eeCVR",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. Example : best -> good, parti -> party"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rER-92If1gU",
        "colab_type": "text"
      },
      "source": [
        "*Lets try to compare stemming vs lemmatization!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31z9spWrXO3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Try to the Words\n",
        "word = 'flying'\n",
        "print('Stemmed:',stemmer.stem(word))\n",
        "print('Lemmatized:',lemmatizer.lemmatize(word,'v'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4apnrXOgnq4",
        "colab_type": "text"
      },
      "source": [
        "*Lemmatizing the text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTuiiPgzYbJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Set Lemmatization Function\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "text_en_lemmatized = [lemmatizer.lemmatize(i, pos='v') for i in text_en_filtered]\n",
        "\n",
        "# Show\n",
        "print('Filtered:',text_en_filtered)\n",
        "print('Stemmed:',text_en_stemmed)\n",
        "print('Lemmatized', text_en_lemmatized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhhRhfb8iRTM",
        "colab_type": "text"
      },
      "source": [
        "### **2. Indonesian Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNp-GXcTiZD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Text (Indonesia)\n",
        "text_id = 'Rakyat memenuhi halaman gedung untuk menyuarakan isi hatinya kepada pemerintah pada tanggal 9 - maret - 2020.'\n",
        "text_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SP_84VmkPNF",
        "colab_type": "text"
      },
      "source": [
        "##### **a. Remove Symbol and Character**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3fs7LR2izRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove Symbol\n",
        "text_id_nosymbol = text_id.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "text_id_nosymbol"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRJ4-WQQkKM1",
        "colab_type": "text"
      },
      "source": [
        "##### **b. Word Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04qa0z5qicTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize Word\n",
        "text_id_tokenized = word_tokenize(text_id_nosymbol)\n",
        "\n",
        "# Show\n",
        "text_id_tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS_VlVzPkULw",
        "colab_type": "text"
      },
      "source": [
        "##### **c. Remove Stopword**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx5RJb6wjeo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Indonesian Stopword\n",
        "stopwords_id = nltk.corpus.stopwords.words('indonesian')\n",
        "\n",
        "# Show Stopwords\n",
        "stopwords_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4zPeY7LjQGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing Stopword\n",
        "text_id_filtered = []\n",
        "for w in text_id_tokenized:\n",
        "    if w not in stopwords_id:\n",
        "        text_id_filtered.append(w)\n",
        "\n",
        "# Show\n",
        "print('Tokenized:',text_id_tokenized)\n",
        "print('Filtered:',text_id_filtered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_s53fxkjrd",
        "colab_type": "text"
      },
      "source": [
        "##### **d. Text Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqPXepEyJ66y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Library for Text Normalization\n",
        "! pip install sastrawi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRIjvSDKCW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2MTopokDr2",
        "colab_type": "text"
      },
      "source": [
        "***Stemming***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFKF1PeKGK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "# Detokenize\n",
        "text_id_detokenized = TreebankWordDetokenizer().detokenize(text_id_filtered)\n",
        "\n",
        "# Create Stemmer Function\n",
        "stemmer_id = StemmerFactory().create_stemmer()\n",
        "text_id_stemmed = stemmer_id.stem(text_id_detokenized)\n",
        "\n",
        "# Show\n",
        "print('Filtered:', text_id_filtered)\n",
        "print('Stemmed:', text_id_stemmed)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}